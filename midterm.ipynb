{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.spatial import distance\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('Volley_Ball_Score.csv')\n",
    "df_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking and changing types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types_dict = {'Performance': 'category', 'Country': 'category'}\n",
    "df_raw = df_raw.astype(data_types_dict)\n",
    "df_raw.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 9 Numerical features and 2 Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.duplicated(keep=False).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.isnull().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.isnull().sum().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 columns have have missing values but none of them have more than 70% of rows with missing data. I will handle the missing values instead of elimination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingno.bar(df_raw.iloc[:, :], color=\"green\", figsize=(10, 5), fontsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingno.matrix(df_raw.iloc[:, :], figsize=(\n",
    "    10, 5), fontsize=12, sparkline=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingno.heatmap(df_raw.iloc[:, :8], figsize=(10, 5), fontsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a deep copy of the dataset and using different duplicate filling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values with kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy(deep=True)\n",
    "df.drop(columns=['Country', 'Performance', 'Unnamed: 0'], inplace=True)\n",
    "\n",
    "\n",
    "knn_imp = KNNImputer(n_neighbors=3)\n",
    "# fit and transform the imputer on the dataset\n",
    "df_knn = pd.DataFrame(knn_imp.fit_transform(df), columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of before and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Player_Score_3'].plot.kde(color='r')\n",
    "df_knn[\"Player_Score_3\"].plot.kde(color='y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Player_Score_4'].plot.kde(color='r')\n",
    "df_knn[\"Player_Score_4\"].plot.kde(color='y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Player_Score_5'].plot.kde(color='r')\n",
    "df_knn[\"Player_Score_5\"].plot.kde(color='y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the jensenshannon distance between the probability distributions before and after imputation\n",
    "\n",
    "counts_imputed, nins_imputed, values_imputed = plt.hist(\n",
    "    df_knn[\"Player_Score_3\"])\n",
    "counts_imputed_probabilities = counts_imputed / counts_imputed.sum()\n",
    "\n",
    "# probability distribution before imputation\n",
    "counts, nins, values = plt.hist(df[\"Player_Score_3\"])\n",
    "counts_probabilities = counts / counts.sum()\n",
    "\n",
    "# pdf distance calculation\n",
    "distance.jensenshannon(counts_imputed_probabilities, counts_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the jensenshannon distance between the probability distributions before and after imputation\n",
    "\n",
    "counts_imputed, nins_imputed, values_imputed = plt.hist(\n",
    "    df_knn[\"Player_Score_4\"])\n",
    "counts_imputed_probabilities = counts_imputed / counts_imputed.sum()\n",
    "\n",
    "# probability distribution before imputation\n",
    "counts, nins, values = plt.hist(df[\"Player_Score_4\"])\n",
    "counts_probabilities = counts / counts.sum()\n",
    "\n",
    "# pdf distance calculation\n",
    "distance.jensenshannon(counts_imputed_probabilities, counts_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the jensenshannon distance between the probability distributions before and after imputation\n",
    "\n",
    "counts_imputed, nins_imputed, values_imputed = plt.hist(\n",
    "    df_knn[\"Player_Score_5\"])\n",
    "counts_imputed_probabilities = counts_imputed / counts_imputed.sum()\n",
    "\n",
    "# probability distribution before imputation\n",
    "counts, nins, values = plt.hist(df[\"Player_Score_5\"])\n",
    "counts_probabilities = counts / counts.sum()\n",
    "\n",
    "# pdf distance calculation\n",
    "distance.jensenshannon(counts_imputed_probabilities, counts_probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values with Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the columns without nan values to impute Player_Score_5.\n",
    "df_regr5 = df_raw[['Player_Score_0', 'Player_Score_1',\n",
    "                   'Player_Score_2', 'Player_Score_5', 'Player_Score_6']]\n",
    "\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "\n",
    "train_data = df_regr5[df_regr5['Player_Score_5'].isnull() == False]\n",
    "test_data = df_regr5[df_regr5['Player_Score_5'].isnull() == True]\n",
    "\n",
    "ps_5_before_imp = train_data['Player_Score_5']\n",
    "\n",
    "y = train_data['Player_Score_5']  # target is \"Player_Score_5\"\n",
    "train_data.drop(\"Player_Score_5\", axis=1, inplace=True)\n",
    "# features are all other features except \"Player_Score_5\"\n",
    "\n",
    "lr_model.fit(train_data, y)\n",
    "\n",
    "test_data.drop(\"Player_Score_5\", axis=1, inplace=True)\n",
    "\n",
    "# infer the missing values with the learned model\n",
    "pred = lr_model.predict(test_data)\n",
    "test_data['Player_Score_5'] = pred\n",
    "\n",
    "ps_5_lr = ps_5_before_imp.append(test_data['Player_Score_5'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the columns without nan values to impute Player_Score_4.\n",
    "df_regr4 = df_raw[['Player_Score_0', 'Player_Score_1',\n",
    "                   'Player_Score_2', 'Player_Score_4', 'Player_Score_6']]\n",
    "\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "\n",
    "train_data = df_regr4[df_regr4['Player_Score_4'].isnull() == False]\n",
    "test_data = df_regr4[df_regr4['Player_Score_4'].isnull() == True]\n",
    "\n",
    "ps_4_before_imp = train_data['Player_Score_4']\n",
    "\n",
    "y = train_data['Player_Score_4']  # target is \"Player_Score_4\"\n",
    "train_data.drop(\"Player_Score_4\", axis=1, inplace=True)\n",
    "# features are all other features except \"Player_Score_4\"\n",
    "\n",
    "lr_model.fit(train_data, y)\n",
    "\n",
    "test_data.drop(\"Player_Score_4\", axis=1, inplace=True)\n",
    "\n",
    "# infer the missing values with the learned model\n",
    "pred = lr_model.predict(test_data)\n",
    "test_data['Player_Score_4'] = pred\n",
    "\n",
    "ps_4_lr = ps_4_before_imp.append(test_data['Player_Score_4'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the columns without nan values to impute Player_Score_3.\n",
    "df_regr3 = df_raw[['Player_Score_0', 'Player_Score_1',\n",
    "                   'Player_Score_2', 'Player_Score_3', 'Player_Score_6']]\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "\n",
    "train_data = df_regr3[df_regr3['Player_Score_3'].isnull() == False]\n",
    "test_data = df_regr3[df_regr3['Player_Score_3'].isnull() == True]\n",
    "\n",
    "ps_3_before_imp = train_data['Player_Score_3']\n",
    "\n",
    "y = train_data['Player_Score_3']  # target is \"Player_Score_3\"\n",
    "train_data.drop(\"Player_Score_3\", axis=1, inplace=True)\n",
    "# features are all other features except \"Player_Score_3\"\n",
    "\n",
    "lr_model.fit(train_data, y)\n",
    "\n",
    "test_data.drop(\"Player_Score_3\", axis=1, inplace=True)\n",
    "\n",
    "# infer the missing values with the learned model\n",
    "pred = lr_model.predict(test_data)\n",
    "test_data['Player_Score_3'] = pred\n",
    "\n",
    "ps_3_lr = ps_3_before_imp.append(test_data['Player_Score_3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualising the distribution before and after imputation\n",
    "\n",
    "# plotting the pdf after imputation\n",
    "df_regr5['Player_Score_5'].plot.kde(color='r')\n",
    "\n",
    "# plotting the pdf before imputation\n",
    "ps_5_lr.plot.kde(color='y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualising the distribution before and after imputation\n",
    "\n",
    "# plotting the pdf after imputation\n",
    "df_regr4['Player_Score_4'].plot.kde(color='r')\n",
    "\n",
    "# plotting the pdf before imputation\n",
    "ps_4_lr.plot.kde(color='y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualising the distribution before and after imputation\n",
    "\n",
    "# plotting the pdf after imputation\n",
    "df_regr3['Player_Score_3'].plot.kde(color='r')\n",
    "\n",
    "# plotting the pdf before imputation\n",
    "ps_3_lr.plot.kde(color='y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking both algorithms and choosing the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the jensenshannon distance between the probability distributions before and after imputation\n",
    "# probability distribution after imputation\n",
    "counts_imputed, nins_imputed, values_imputed = plt.hist(ps_3_lr)\n",
    "counts_imputed_probabilities = counts_imputed / counts_imputed.sum()\n",
    "\n",
    "# probability distribution before imputation\n",
    "counts, nins, values = plt.hist(ps_3_before_imp)\n",
    "counts_probabilities = counts / counts.sum()\n",
    "\n",
    "# pdf distance calculation\n",
    "distance.jensenshannon(counts_imputed_probabilities, counts_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the jensenshannon distance between the probability distributions before and after imputation\n",
    "# probability distribution after imputation\n",
    "counts_imputed, nins_imputed, values_imputed = plt.hist(ps_4_lr)\n",
    "counts_imputed_probabilities = counts_imputed / counts_imputed.sum()\n",
    "\n",
    "# probability distribution before imputation\n",
    "counts, nins, values = plt.hist(ps_4_before_imp)\n",
    "counts_probabilities = counts / counts.sum()\n",
    "\n",
    "# pdf distance calculation\n",
    "distance.jensenshannon(counts_imputed_probabilities, counts_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the jensenshannon distance between the probability distributions before and after imputation\n",
    "# probability distribution after imputation\n",
    "counts_imputed, nins_imputed, values_imputed = plt.hist(ps_5_lr)\n",
    "counts_imputed_probabilities = counts_imputed / counts_imputed.sum()\n",
    "\n",
    "# probability distribution before imputation\n",
    "counts, nins, values = plt.hist(ps_5_before_imp)\n",
    "counts_probabilities = counts / counts.sum()\n",
    "\n",
    "# pdf distance calculation\n",
    "distance.jensenshannon(counts_imputed_probabilities, counts_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the pdf after imputation using knn\n",
    "df_knn['Player_Score_3'].plot.kde(color='r')\n",
    "\n",
    "# plotting the pdf before imputation using linear regression\n",
    "ps_3_lr.plot.kde(color='y')\n",
    "\n",
    "# plotting the pdf before imputation without imputation\n",
    "ps_3_before_imp.plot.kde(color='b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the pdf after imputation using knn\n",
    "df_knn['Player_Score_4'].plot.kde(color='r')\n",
    "\n",
    "# plotting the pdf before imputation using linear regression\n",
    "ps_4_lr.plot.kde(color='y')\n",
    "\n",
    "# plotting the pdf before imputation without imputation\n",
    "ps_4_before_imp.plot.kde(color='b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the pdf after imputation using knn\n",
    "df_knn['Player_Score_5'].plot.kde(color='r')\n",
    "\n",
    "# plotting the pdf before imputation using linear regression\n",
    "ps_5_lr.plot.kde(color='y')\n",
    "\n",
    "# plotting the pdf before imputation without imputation\n",
    "ps_5_before_imp.plot.kde(color='b')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Verify the features values distribution of the numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn.iloc[:, :].hist(figsize=(15, 15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_df(data, rows_max, cols_max):\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    row = 1\n",
    "    col = 1\n",
    "    fig = make_subplots(rows=rows_max, cols=cols_max)\n",
    "    for i in data.columns:\n",
    "        if row == rows_max:\n",
    "            fig.append_trace(go.Histogram(x=data[i], name=i), row=row, col=col)\n",
    "            col += 1\n",
    "            row = 1\n",
    "            continue\n",
    "        fig.append_trace(go.Histogram(x=data[i], name=i), row=row, col=col)\n",
    "        row += 1\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "hist_df(df_knn, rows_max=2, cols_max=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Is features transformation necessary for the numerical variables? Let‚Äôs take into account that we are preparing the dataset for a Linear Regression task, with the goal of building a \"Score\" predictive model. If transformation is necessary, after justifying your choices, do proceed as described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using minmax scaler from sklearn.preprocessing to scale the numerical columns in the dataframe.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "numerical_features = ['Player_Score_0', 'Player_Score_1', 'Player_Score_2',\n",
    "                      'Player_Score_3', 'Player_Score_4', 'Player_Score_5', 'Player_Score_6', 'Score']\n",
    "\n",
    "\n",
    "df_scaled = df_knn[numerical_features]\n",
    "transformer = RobustScaler().fit(df_scaled)\n",
    "transformer.transform(df_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df(df_scaled, 2, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Verify the presence of outliers and eventually handle them. Justify your choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_scaled.columns:\n",
    "    # discovering outliers with IQR-score\n",
    "    Q1 = df_scaled[i].quantile(0.05)\n",
    "    Q3 = df_scaled[i].quantile(0.95)\n",
    "    IQR = Q3 - Q1\n",
    "    print(IQR)\n",
    "\n",
    "    # DROP\n",
    "    logical_index_not_outliers = (df_scaled[i] > (\n",
    "        Q1 - 1.5 * IQR)) & (df_scaled[i] < (Q3 + 1.5 * IQR))\n",
    "    df_scaled = df_scaled[logical_index_not_outliers]\n",
    "    # CAP\n",
    "    df_scaled.loc[(df_scaled[i] < Q1), i] = Q1\n",
    "    df_scaled.loc[(df_scaled[i] > Q3), i] = Q3\n",
    "df_scaled.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no outliers to be handled since the robust scaling handled them, hence nothing was dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Is encoding necessary for the categorical variables? If yes, which kind of encoding? Specify your choices, justify them and perform categorical data encoding, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode using sklearn\n",
    "df_scaled['Country'] = df_raw['Country']\n",
    "df_scaled['Performance'] = df_raw['Performance']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace a categorical value with a specific numeric one\n",
    "dictionary = {\"Performance\": {'below_average': 0, 'neutral': 1,\n",
    "                              'average': 2, 'above_average': 3, 'extraordinary': 4}}\n",
    "df_scaled.replace(dictionary, inplace=True)\n",
    "# Replace a categorical value with a specific numeric one\n",
    "dictionary = {'France': 0, 'Finland': 1, 'Germany': 2,\n",
    "              'Norway': 3, 'Switzerland': 4, 'The_Netherlands': 5, 'Italy': 6}\n",
    "df_scaled.replace(dictionary, inplace=True)\n",
    "\n",
    "display(df_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Increase the dimensionality of the dataset introducing Polynomial Features ‚Äì degree = 3 (continuous variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['Player_Score_0', 'Player_Score_1', 'Player_Score_2',\n",
    "                      'Player_Score_3', 'Player_Score_4', 'Player_Score_5', 'Player_Score_6']\n",
    "\n",
    "df_dimensionality = df_scaled[numerical_features].copy(deep=True)\n",
    "df_dimensionality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "poly.fit(df_dimensionality)\n",
    "X_poly = poly.transform(df_dimensionality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled['Performance'] = df_scaled['Performance'].astype('float64')\n",
    "df_scaled.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Eventually include any other transformation which might be necessary/appropriate and justify your choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Features Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Perform One Way ANOVA and test the relationship between variable Country and Score. Eventually, consider the possibility to remove the feature. Justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = pd.unique(df_scaled.Country.values)\n",
    "groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = pd.unique(df_scaled.Country.values)\n",
    "d_data = {grp: df_scaled['Score'][df_scaled.Country == grp] for grp in groups}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95% confidence test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using scipy f_oneway to calculate the p value\n",
    "# 'France':0, 'Finland':1, 'Germany':2, 'Norway':3, 'Switzerland':4, 'The_Netherlands':5, 'Italy':6\n",
    "from scipy import stats\n",
    "Fcritical = 3.179\n",
    "F, p = stats.f_oneway(d_data[0], d_data[1], d_data[2],\n",
    "                      d_data[3], d_data[4], d_data[5], d_data[6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if F > Fcritical:\n",
    "    print(\"reject null hypothesis H0\")\n",
    "else:\n",
    "    print(\"accept null hypothesis H0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We accept the null hypothesis meaning that it doesnt exist variance between the groups, I won't include this feature for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_scaled.copy(deep=True)\n",
    "df_model = df_model.drop(columns='Country')\n",
    "df_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Perform Features Selection and visualize the features which have been selected. Select one appropriate methodology for features selection and justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_model.copy(deep=True)\n",
    "df_train.drop(columns='Score', inplace=True)\n",
    "y = df_model['Score'].copy(deep=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train, y, random_state=23102002, test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use f_classif (the default) and SelectPercentile to select 50% of features\n",
    "select = SelectPercentile(percentile=50)\n",
    "select.fit(X_train, y_train)\n",
    "y_train = y_train.astype('int')\n",
    "# transform train set\n",
    "X_train_selected = select.transform(X_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = select.get_support()\n",
    "\n",
    "print(mask)\n",
    "\n",
    "# visualize the mask -- black is True, white is False\n",
    "plt.matshow(mask.reshape(1, -1), cmap='gray_r')\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.yticks(())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_model.copy(deep=True)\n",
    "df_train.drop(columns='Score', inplace=True)\n",
    "y = df_model['Score'].copy(deep=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train, y, random_state=23102002, test_size=.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# select the k best features based on ANOVA F-value between label/feature (classification tasks)\n",
    "# ANOVA F-value tells us if the tuple of variables (label-feature) are jointly significant.\n",
    "k_best = SelectKBest(k=6)  # Select features according to the k highest score\n",
    "fit = k_best.fit(X_train, y_train)\n",
    "\n",
    "# transform training set\n",
    "X_train_selected = k_best.transform(X_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = k_best.get_support()\n",
    "print(mask)\n",
    "# visualize the mask -- black is True, white is False\n",
    "plt.matshow(mask.reshape(1, -1), cmap='gray_r')\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.yticks(())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Won't use this feature selection method because it's not really useful for the amount of features we have and I think we will need most of the features present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_model.copy(deep=True)\n",
    "df_train.drop(columns='Score', inplace=True)\n",
    "y = df_model['Score'].copy(deep=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train, y, random_state=23102002, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = SelectFromModel(RandomForestClassifier(\n",
    "    n_estimators=100, random_state=24))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select.fit(X_train, y_train)\n",
    "X_train_l1 = select.transform(X_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print(\"X_train_l1.shape: {}\".format(X_train_l1.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = select.get_support()\n",
    "# visualize the mask -- black is True, white is False\n",
    "plt.matshow(mask.reshape(1, -1), cmap='gray_r')\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.yticks(())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_model.copy(deep=True)\n",
    "df_train.drop(columns='Score', inplace=True)\n",
    "y = df_model['Score'].copy(deep=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train, y, random_state=23102002, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE=Feature ranking with recursive feature elimination.\n",
    "\n",
    "select = RFE(RandomForestClassifier(n_estimators=100,\n",
    "             random_state=24), n_features_to_select=5)\n",
    "\n",
    "select.fit(X_train, y_train)\n",
    "# visualize the selected features:\n",
    "mask = select.get_support()  # mask of selected features\n",
    "plt.matshow(mask.reshape(1, -1), cmap='gray_r')\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.yticks(())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def gradient_descent_2(eta, X, Y, numIterations):\n",
    "    s = X.shape[0]\n",
    "    theta = np.ones(9)\n",
    "    X_transpose = X.transpose()\n",
    "    for iter in range(0, numIterations):\n",
    "        hypothesis = np.dot(X, theta)\n",
    "        loss = hypothesis - Y\n",
    "        gradient = np.dot(X_transpose, loss) / s\n",
    "        theta = theta - eta * gradient\n",
    "\n",
    "        y_predict = theta[0] + theta[1]*X_train\n",
    "        plt.plot(X_train, y_predict, 'r')\n",
    "\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rmse = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Train a Multiple Linear Regression model, using the Sklearn implementation of Linear Regression to find the best ùúΩ vector. Use all the transformed features, excluding the derived polynomial features. Evaluate the model with the learned ùúΩ on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_model.copy(deep=True)\n",
    "df_train.drop(columns='Score', inplace=True)\n",
    "y=df_model['Score'].copy(deep=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train, y, random_state=24)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "\n",
    "lin_reg.fit(X_train, y_train)\n",
    "print(\"Intercept={}, Slope={}\".format(lin_reg.intercept_, lin_reg.coef_))\n",
    "y_predict = lin_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(y_test)\n",
    "sns.lineplot(y_predict, color='r', label=\"predictions\")\n",
    "plt.xlabel('size')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test RMSE={}\".format(np.sqrt(mean_squared_error(y_test, y_predict))))\n",
    "print(\"test R2={}\".format(r2_score(y_test, y_predict)))\n",
    "\n",
    "print(\"train RMSE={}\".format(\n",
    "    np.sqrt(mean_squared_error(y_train, lin_reg.predict(X_train)))))\n",
    "print(\"train R2={}\".format(r2_score(y_train, lin_reg.predict(X_train))))\n",
    "\n",
    "final_rmse.append({'Linear Regression test': np.sqrt(mean_squared_error(y_test, y_predict))})\n",
    "final_rmse.append({'Linear Regression train': np.sqrt(mean_squared_error(y_train, lin_reg.predict(X_train)))})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Use all the transformed features, excluding the derived polynomial features, to identify the best values of ùúΩ by means of a Batch Gradient Descent procedure. Identify the best values of ùúº (starting with an initial value of ùúº = 0.1 ). Evaluate the model with the trained ùúΩ on the test set. Plot the train and the test error for increasing number of iterations of the Gradient Descent procedure (with the best value of ùúº). Provide a comment of the plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_model.copy(deep=True)\n",
    "df_train.drop(columns='Score', inplace=True)\n",
    "y=df_model['Score'].copy(deep=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train, y, random_state=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.c_[np.ones(X_train.shape[0]), X_train]  # insert column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, q = np.shape(X_train)\n",
    "eta = 0.01  # learning rate\n",
    "theta = gradient_descent_2(eta, X_train, y_train, 100)\n",
    "print(theta)\n",
    "sns.scatterplot(x=X_train[:, 1], y=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, q = np.shape(X_train)\n",
    "eta = 0.1  # learning rate\n",
    "theta = gradient_descent_2(eta, X_train, y_train, 100)\n",
    "print(theta)\n",
    "sns.scatterplot(x=X_train[:, 1], y=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "s, q = np.shape(X_train)\n",
    "eta = 0.2  # learning rate\n",
    "theta = gradient_descent_2(eta, X_train, y_train, 100)\n",
    "print(theta)\n",
    "sns.scatterplot(x=X_train[:, 1], y=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "s, q = np.shape(X_train)\n",
    "eta = 0.3  # learning rate\n",
    "theta = gradient_descent_2(eta, X_train, y_train, 100)\n",
    "print(theta)\n",
    "sns.scatterplot(x=X_train[:, 1], y=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "s, q = np.shape(X_train)\n",
    "eta = 0.4  # learning rate\n",
    "theta = gradient_descent_2(eta, X_train, y_train, 100)\n",
    "print(theta)\n",
    "sns.scatterplot(x=X_train[:, 1], y=y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sweet spot is eta = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Use the complete set of features, including the derived polynomial features. Train a Multiple Linear Regression model, using the Sklearn implementation of Linear Regression to find the best ùúΩ vector. Evaluate the model with the learned ùúΩ on the test set. Plot the train and the test error for increasing the size of the train-set (with the best value of ùúº). Provide a comment of the plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_model.copy(deep=True)\n",
    "y=df_model['Score'].copy(deep=True)\n",
    "df_train.drop(columns=['Score'], inplace=True)\n",
    "\n",
    "df_poly = pd.DataFrame(X_poly)\n",
    "\n",
    "df_new_train = pd.concat([df_poly, df_train], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_new_train, y, random_state=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(y_train)\n",
    "sns.lineplot(lin_reg.predict(X_test), label='prediction', color='r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Use the complete set of features, including the derived polynomial features. Train a Ridge Regression model identifying the best value of the learning rate ùú∂ that allows the model to achieve the best generalization performances. Evaluate the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_model.copy(deep=True)\n",
    "y=df_model['Score'].copy(deep=True)\n",
    "df_train.drop(columns=['Score'], inplace=True)\n",
    "\n",
    "df_poly = pd.DataFrame(X_poly)\n",
    "\n",
    "df_new_train = pd.concat([df_poly, df_train], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_new_train, y, random_state=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.5, 1, 2, 3, 4, 5, 6, 7, 10, 11,\n",
    "          20, 30, 40, 50, 60, 70, 80, 90, 100,150,200]\n",
    "rmse_values = []\n",
    "rmse_values_train =[]\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    y_predict = ridge_model.predict(X_test)\n",
    "    rmse_values.append(np.sqrt(mean_squared_error(y_test, y_predict)))\n",
    "    rmse_values_train.append(np.sqrt(mean_squared_error(y_train, ridge_model.predict(X_train))))\n",
    "\n",
    "plt.plot(alphas, rmse_values)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel(\"RMSE\")\n",
    "\n",
    "for i, j in zip(alphas, rmse_values):\n",
    "    print('Alpha = {}, RMSE = {}'.format(i, j))\n",
    "\n",
    "print(\"Minimum test-RMSE = {}\".format(np.min(rmse_values)))\n",
    "\n",
    "\n",
    "final_rmse.append({'Ridge Regression test': np.min(rmse_values)})\n",
    "final_rmse.append({'Ridge Regression train': np.min(rmse_values_train)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Use the complete set of features, including the derived polynomial features. Train a Linear Regression model with Lasso regularization. Comment on the importance of each feature given the related trained parameter value of the trained model. Also, verify the number of features selected (related coefficient ùúΩ different from zero) with different values of ùõº.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_model.copy(deep=True)\n",
    "y=df_model['Score'].copy(deep=True)\n",
    "df_train.drop(columns=['Score'], inplace=True)\n",
    "\n",
    "df_poly = pd.DataFrame(X_poly)\n",
    "\n",
    "df_new_train = pd.concat([df_poly, df_train], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_new_train, y, random_state=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.001, 0.002, 0.003, 0.004, 0.005, 0.006,\n",
    "          0.008, 0.1, 0.2, 1, 1.4, 1.45, 1.5, 1.6, 2, 3]\n",
    "rmse_values = []\n",
    "rmse_values_train = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso_model = Lasso(alpha=alpha)\n",
    "    lasso_model.fit(X_train, y_train)\n",
    "    y_predict = lasso_model.predict(X_test)\n",
    "    rmse_values.append(np.sqrt(mean_squared_error(y_test, y_predict)))\n",
    "    rmse_values_train.append(np.sqrt(mean_squared_error(y_train, ridge_model.predict(X_train))))\n",
    "\n",
    "plt.plot(alphas, rmse_values)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel(\"RMSE\")\n",
    "\n",
    "for i, j in zip(alphas, rmse_values):\n",
    "    print('Alpha = {}, RMSE = {}'.format(i, j))\n",
    "\n",
    "print(\"Minimum test-RMSE = {}\".format(np.min(rmse_values)))\n",
    "\n",
    "final_rmse.append({'Lasso Regression test': np.min(rmse_values)})\n",
    "final_rmse.append({'Lasso Regression train': np.min(rmse_values_train)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Use the subset of features selected in the Feature Selection task (question 3b). Train a Multiple Linear Regression model using the Sklearn implementation of Linear Regression to find the best ùúΩ vector. Evaluate the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features = df_model.copy(deep=True)\n",
    "y=df_model['Score'].copy(deep=True)\n",
    "df_train_features.drop(columns=['Score', 'Performance'], inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train_features, y, random_state=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "\n",
    "lin_reg.fit(X_train, y_train)\n",
    "print(\"Intercept={}, Slope={}\".format(lin_reg.intercept_, lin_reg.coef_))\n",
    "\n",
    "y_predict = lin_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(y_test)\n",
    "sns.lineplot(y_predict, color='r', label=\"predictions\")\n",
    "plt.xlabel('size')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test RMSE={}\".format(np.sqrt(mean_squared_error(y_test, y_predict))))\n",
    "print(\"test R2={}\".format(r2_score(y_test, y_predict)))\n",
    "\n",
    "print(\"train RMSE={}\".format(\n",
    "    np.sqrt(mean_squared_error(y_train, lin_reg.predict(X_train)))))\n",
    "print(\"train R2={}\".format(r2_score(y_train, lin_reg.predict(X_train))))\n",
    "\n",
    "\n",
    "\n",
    "final_rmse.append({'Linear Regression features test': np.sqrt(mean_squared_error(y_test, y_predict))})\n",
    "final_rmse.append({'Linear Regression features train': np.sqrt(mean_squared_error(y_train, lin_reg.predict(X_train)))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g) Create a table with the evaluation results obtained from all the models above on both the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(model, X, y, model_type, c):\n",
    "  \n",
    "  X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "  train_errors, test_errors = [], []\n",
    "  \n",
    "  for m in range(1, len(X_train)):\n",
    "\n",
    "    model.fit(X_train[:m], y_train[:m])\n",
    "    y_train_predict = model.predict(X_train[:m])\n",
    "    y_test_predict = model.predict(X_test)\n",
    "\n",
    "    train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "    test_errors.append(mean_squared_error(y_test, y_test_predict))\n",
    "\n",
    "  plt.plot(np.sqrt(train_errors), 'b', linewidth=2, label=\"train_\"+model_type)\n",
    "  plt.plot(np.sqrt(test_errors), 'g' , linewidth=3, label=\"test_\"+model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_model.copy(deep=True)\n",
    "y=df_model['Score'].copy(deep=True)\n",
    "df_train.drop(columns=['Score'], inplace=True)\n",
    "\n",
    "df_poly = pd.DataFrame(X_poly)\n",
    "\n",
    "df_new_train = pd.concat([df_poly, df_train], axis=1).copy(deep=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train, y, random_state=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "lin_reg = LinearRegression()\n",
    "plot_learning_curves(lin_reg, df_train, y, \"LinearRegression\", 'g')\n",
    "plt.title('Linear Regression, learning curve')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_model.copy(deep=True)\n",
    "y=df_model['Score'].copy(deep=True)\n",
    "df_train.drop(columns=['Score'], inplace=True)\n",
    "\n",
    "df_poly = pd.DataFrame(X_poly)\n",
    "\n",
    "df_new_train = pd.concat([df_poly, df_train], axis=1).copy(deep=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_new_train, y, random_state=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "lin_reg = Lasso(alpha=3)\n",
    "plot_learning_curves(lin_reg, df_new_train, y, \"Lasso\", 'g')\n",
    "plt.title('Lasso Regression, learning curve (alpha=3)')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features = df_model.copy(deep=True)\n",
    "y=df_model['Score'].copy(deep=True)\n",
    "df_train_features.drop(columns=['Score','Performance'], inplace=True)\n",
    "\n",
    "df_poly = pd.DataFrame(X_poly)\n",
    "\n",
    "df_new_train = pd.concat([df_poly, df_train_features], axis=1).copy(deep=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train_features, y, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "lin_reg = Ridge(alpha=60, solver=\"cholesky\")\n",
    "plot_learning_curves(lin_reg, df_new_train, y, \"Ridge\", 'g')\n",
    "plt.title('Ridge Regression, learning curve (alpha=60)')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features = df_model.copy(deep=True)\n",
    "y=df_model['Score'].copy(deep=True)\n",
    "df_train_features.drop(columns=['Score','Performance'], inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train_features, y, random_state=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "lin_reg = LinearRegression()\n",
    "plot_learning_curves(lin_reg, df_train_features, y, \"LinearRegression\", 'g')\n",
    "plt.title('Linear Regression, Features, learning curve')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (h) Compare and discuss the results obtained above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1eaddc2cb33274cb8d07c95d963fb7284d371bd13f214bdf9300628d93ff1773"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
